{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hOo48Oi7xla",
        "outputId": "e2602572-7ba7-4249-a68d-e8a6c5b10f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Libraries\n",
        "%pip install pandas faiss-cpu sentence-transformers torch transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdO5y3jb9aH-",
        "outputId": "538b1b6e-bad0-47b6-990c-304fab5d2655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame shape: (100, 10)\n",
            "\n",
            "Columns: ['Source URL', 'Scheme Name', 'Ministry', 'Description', 'Category', 'Eligibility', 'Benefits', 'Application Process', 'Documents', 'Last Updated']\n",
            "\n",
            "First few rows:\n",
            "                                                      Source URL  \\\n",
            "0                 https://www.myscheme.gov.in/schemes/fadsp1012e   \n",
            "1                   https://www.myscheme.gov.in/schemes/icmr-pdf   \n",
            "2                     https://www.myscheme.gov.in/schemes/tkgthe   \n",
            "3                    https://www.myscheme.gov.in/schemes/skerala   \n",
            "4  https://www.myscheme.gov.in/schemes/sgassobcaniphsaislecxixii   \n",
            "\n",
            "                                                             Scheme Name  \\\n",
            "0  Financial Assistance To Disabled Students Pursuing (10th, 11th, 12...   \n",
            "1                                         ICMR- Post Doctoral Fellowship   \n",
            "2                     Tool Kit Grant for Traditional Handicrafts Experts   \n",
            "3                                                        Snehasanthwanam   \n",
            "4  Scheme for Grant of Additional Scholarship to the Students of Othe...   \n",
            "\n",
            "                              Ministry  \\\n",
            "0                               Kerala   \n",
            "1  Ministry Of Health & Family Welfare   \n",
            "2                               Kerala   \n",
            "3                               Kerala   \n",
            "4          Andaman and Nicobar Islands   \n",
            "\n",
            "                                                             Description  \\\n",
            "0  The scheme “Financial Assistance to Disabled Students Pursuing (10...   \n",
            "1  ICMR- Post Doctoral Fellowship (ICMR-PDF) Scheme is being institut...   \n",
            "2  The “Tool Kit Grant for Traditional Handicrafts Experts” scheme is...   \n",
            "3  The Government of Kerala has launched the “Snehasanthwanam” scheme...   \n",
            "4  The objective of the scheme is to provide additional scholarship t...   \n",
            "\n",
            "                                                                Category  \\\n",
            "0                 APL, BPL, Disabled, Financial Assistance, PwD, Student   \n",
            "1                         Fellowship, ICMR, PDF, Post Doctoral, Research   \n",
            "2  Craftsman, Grant, Handicrafts, OBC, Self-employment, Stipend, Tool...   \n",
            "3  Bedridden, Educational Assistance, Endosulfan, Financial Assistanc...   \n",
            "4                                Higher Study, OBC, Scholarship, Student   \n",
            "\n",
            "                                                             Eligibility  \\\n",
            "0  The applicant should be a resident of Kerala State. The differentl...   \n",
            "1  ICMR-PDF is open to Indian nationals only. Fresh PhDs/MD/MS within...   \n",
            "2  1. The applicant should be a permanent resident of the Kerala Stat...   \n",
            "3  The applicant should be a resident of Kerala State. The applicant ...   \n",
            "4  The scholarship shall be open to students belonging to a non-cream...   \n",
            "\n",
            "                                                                Benefits  \\\n",
            "0  Sl. No. Class/Course Course Fee Assistance Exam Fee Assistance Reg...   \n",
            "1  ICMR-PDFs will be paid a consolidated fellowship of ₹65,000/- per ...   \n",
            "2  1. The training costs, stipend (if found necessary), and tool kit ...   \n",
            "3  Under the scheme, financial assistance of ₹2200/- is given to the ...   \n",
            "4  Value of Scholarship: Under this scheme, an amount of ₹1000/- per ...   \n",
            "\n",
            "                                                     Application Process  \\\n",
            "0  Step 01: To avail of the benefits of the scheme, the applicant nee...   \n",
            "1  Last Date of Application (30th June and 31st December Every Year) ...   \n",
            "2  Step 01: The application will be invited through public notificati...   \n",
            "3  To avail of the benefits of the scheme, the application duly fille...   \n",
            "4  Step 01: The students applying for the scholarship for the first t...   \n",
            "\n",
            "                                                               Documents  \\\n",
            "0  1. Identity proof 2. Passport-size photo 3. Educational Certificat...   \n",
            "1  1. Passport-size Photograph 2. Attested copy of School Leaving/ Hi...   \n",
            "2  Identity proof Photo of the applicant Caste Certificate Income Cer...   \n",
            "3  1. Identity proof i.e. Aadhaar card 2. Passport size photograph 3....   \n",
            "4  Passport-size photograph Certificates of Class X/XII Diploma/Degre...   \n",
            "\n",
            "  Last Updated  \n",
            "0          N/A  \n",
            "1          N/A  \n",
            "2          N/A  \n",
            "3          N/A  \n",
            "4          N/A  \n",
            "\n",
            "Checking for missing values:\n",
            "Source URL             0\n",
            "Scheme Name            0\n",
            "Ministry               0\n",
            "Description            0\n",
            "Category               0\n",
            "Eligibility            0\n",
            "Benefits               0\n",
            "Application Process    0\n",
            "Documents              0\n",
            "Last Updated           0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and Load Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Configure logging for this part\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_FILE = \"myscheme_schemes_data_FINAL.json\"\n",
        "# --- ---\n",
        "\n",
        "logging.info(\"Loading data...\")\n",
        "try:\n",
        "    # Load the JSON data into a pandas DataFrame\n",
        "    df_schemes = pd.read_json(DATA_FILE, orient='records')\n",
        "\n",
        "    # Basic Data Exploration\n",
        "    logging.info(f\"Data loaded successfully from {DATA_FILE}\")\n",
        "    print(f\"DataFrame shape: {df_schemes.shape}\")\n",
        "    print(\"\\nColumns:\", df_schemes.columns.tolist())\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    # Display head with potential truncation for long text fields\n",
        "    with pd.option_context('display.max_colwidth', 70): # Limit column width for display\n",
        "        print(df_schemes.head())\n",
        "    print(\"\\nChecking for missing values:\")\n",
        "    print(df_schemes.isnull().sum())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logging.error(f\"Error: Data file not found at {DATA_FILE}\")\n",
        "    print(f\"Error: Could not find the data file '{DATA_FILE}'. Please ensure it's in the correct directory.\")\n",
        "    # Optionally raise error to stop execution\n",
        "    # raise\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading or processing data: {e}\", exc_info=True)\n",
        "    print(f\"An error occurred while loading the data: {e}\")\n",
        "    # raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Gxt2LM9987",
        "outputId": "5fd000d3-df29-47ef-ef3c-5ea85d728961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total text chunks created: 499\n",
            "Total metadata entries created: 499\n",
            "\n",
            "--- Example Chunk 1 (Overview) ---\n",
            "Scheme Name: Financial Assistance To Disabled Students Pursuing (10th, 11th, 12th Equivalent Exams) Ministry or Department: Kerala Description: The scheme “Financial Assistance to Disabled Students Pursuing (10th, 11th, 12th Equivalent Exams)” was launched by the Department of Social Justice, Government of Kerala. Relevant Categories/Tags: APL, BPL, Disabled, Financial Assistance, PwD, Student\n",
            "\n",
            "--- Example Metadata 1 (Overview) ---\n",
            "{'scheme_name': 'Financial Assistance To Disabled Students Pursuing (10th, 11th, 12th Equivalent Exams)', 'source_url': 'https://www.myscheme.gov.in/schemes/fadsp1012e', 'field': 'Overview', 'original_index': 0}\n",
            "\n",
            "--- Example Chunk 2 (First Detail Field Found) ---\n",
            "Scheme: Financial Assistance To Disabled Students Pursuing (10th, 11th, 12th Equivalent Exams) | Eligibility: The applicant should be a resident of Kerala State. The differently abled students with 40% or more disability are eligible to apply under the scheme. Financial assistance will be provided to students falling in Above Poverty Line (APL) as well as Below Poverty Line (BPL) categories.\n",
            "\n",
            "--- Example Metadata 2 (First Detail Field Found) ---\n",
            "{'scheme_name': 'Financial Assistance To Disabled Students Pursuing (10th, 11th, 12th Equivalent Exams)', 'source_url': 'https://www.myscheme.gov.in/schemes/fadsp1012e', 'field': 'Eligibility', 'original_index': 0}\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Data Preprocessing and Chunking (Revised Concept)\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import pandas as pd # Assuming df_schemes is a pandas DataFrame\n",
        "\n",
        "# Hypothetical Text Splitter Function (You'd need to implement or import this)\n",
        "# from your_splitter_library import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\n",
        "\n",
        "def hypothetical_split_text(text, chunk_size=700, chunk_overlap=70):\n",
        "    # *** This is a placeholder - Implement actual splitting logic ***\n",
        "    # Example: Simple splitting for illustration, NOT production quality.\n",
        "    # Replace with RecursiveCharacterTextSplitter or similar.\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    else:\n",
        "        parts = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            parts.append(text[start:end])\n",
        "            start += chunk_size - chunk_overlap # Move start index back by overlap\n",
        "            if start < 0: start = 0 # Avoid negative index if overlap > size\n",
        "        # Filter out potential empty strings from splitting\n",
        "        return [part for part in parts if part.strip()]\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "MIN_CHUNK_LENGTH = 15 # Reduced minimum length\n",
        "DETAIL_COLUMNS = [\"Eligibility\", \"Benefits\", \"Application Process\", \"Documents\"]\n",
        "MAX_FIELD_LENGTH_BEFORE_SPLIT = 300 # Example threshold\n",
        "CHUNK_SIZE_FOR_SPLIT = 700 # Example target size for split parts\n",
        "CHUNK_OVERLAP_FOR_SPLIT = 70 # Example overlap\n",
        "\n",
        "logging.info(\"Starting data preprocessing and chunking (v2)...\")\n",
        "\n",
        "# --- Load DataFrame (Assuming df_schemes is loaded previously) ---\n",
        "# Example: df_schemes = pd.read_json(\"myscheme_schemes_data_FINAL.json\")\n",
        "# Or loaded from previous cell output\n",
        "\n",
        "chunks = []\n",
        "chunk_metadata = []\n",
        "\n",
        "# Iterate through each scheme\n",
        "for index, row in df_schemes.iterrows():\n",
        "    scheme_name = row.get('Scheme Name', 'Unknown Scheme')\n",
        "    source_url = row.get('Source URL', 'N/A')\n",
        "    ministry = row.get('Ministry', 'N/A')\n",
        "    description = row.get('Description', '') # Use empty string if missing\n",
        "    category = row.get('Category', '')\n",
        "\n",
        "    # --- Chunk 1: Scheme Overview ---\n",
        "    # (Keep this mostly as before, maybe check description length too)\n",
        "    overview_text = (\n",
        "        f\"Scheme Name: {scheme_name}\\n\"\n",
        "        f\"Ministry or Department: {ministry}\\n\"\n",
        "        f\"Description: {description}\\n\" # Ensure description is not None\n",
        "        f\"Relevant Categories/Tags: {category}\" # Ensure category is not None\n",
        "    )\n",
        "    overview_text_cleaned = ' '.join(overview_text.split())\n",
        "    if len(overview_text_cleaned) > MIN_CHUNK_LENGTH: # Basic check\n",
        "        chunks.append(overview_text_cleaned)\n",
        "        chunk_metadata.append({\n",
        "            \"scheme_name\": scheme_name, \"source_url\": source_url,\n",
        "            \"field\": \"Overview\", \"original_index\": index\n",
        "        })\n",
        "\n",
        "    # --- Process Detail Fields ---\n",
        "    for col_name in DETAIL_COLUMNS:\n",
        "        detail_text = row.get(col_name)\n",
        "        original_field_content = \"\" # Store original content before splitting\n",
        "\n",
        "        is_valid_text = False\n",
        "        if isinstance(detail_text, str):\n",
        "            detail_text = detail_text.strip()\n",
        "            if detail_text and not detail_text.startswith(\"N/A\") and len(detail_text) >= MIN_CHUNK_LENGTH:\n",
        "                is_valid_text = True\n",
        "                original_field_content = detail_text # Store the valid text\n",
        "\n",
        "        if is_valid_text:\n",
        "            # *** NEW: Check if splitting is needed ***\n",
        "            if len(original_field_content) > MAX_FIELD_LENGTH_BEFORE_SPLIT:\n",
        "                logging.debug(f\"Splitting long field '{col_name}' for scheme '{scheme_name[:30]}...'\")\n",
        "                # Use the text splitter\n",
        "                split_parts = hypothetical_split_text(\n",
        "                    original_field_content,\n",
        "                    chunk_size=CHUNK_SIZE_FOR_SPLIT,\n",
        "                    chunk_overlap=CHUNK_OVERLAP_FOR_SPLIT\n",
        "                )\n",
        "\n",
        "                for i, part in enumerate(split_parts):\n",
        "                    part = part.strip()\n",
        "                    if len(part) >= MIN_CHUNK_LENGTH: # Check length of split part\n",
        "                         chunk_text = f\"Scheme: {scheme_name} | {col_name} (Part {i+1}): {part}\"\n",
        "                         chunks.append(chunk_text)\n",
        "                         chunk_metadata.append({\n",
        "                             \"scheme_name\": scheme_name, \"source_url\": source_url,\n",
        "                             \"field\": col_name, \"original_index\": index, \"part_num\": i+1\n",
        "                         })\n",
        "                    else:\n",
        "                         logging.debug(f\"Skipping short split part {i+1} for '{col_name}' in '{scheme_name[:30]}...'\")\n",
        "\n",
        "            else:\n",
        "                # Field is short enough, create a single chunk as before\n",
        "                chunk_text = f\"Scheme: {scheme_name} | {col_name}: {original_field_content}\"\n",
        "                chunks.append(chunk_text)\n",
        "                chunk_metadata.append({\n",
        "                    \"scheme_name\": scheme_name, \"source_url\": source_url,\n",
        "                    \"field\": col_name, \"original_index\": index\n",
        "                })\n",
        "        else:\n",
        "             logging.debug(f\"Skipping chunk creation for scheme '{scheme_name[:30]}...' field '{col_name}' due to invalid/short content.\")\n",
        "\n",
        "\n",
        "# --- Verification and Summary ---\n",
        "logging.info(f\"Chunking complete. Generated {len(chunks)} chunks from {len(df_schemes)} schemes.\")\n",
        "# (Rest of the verification logic from your original Cell 3)\n",
        "# ...\n",
        "\n",
        "if len(chunks) != len(chunk_metadata):\n",
        "     logging.critical(\"CRITICAL ERROR: Mismatch between chunks count ({}) and metadata count ({})!\".format(len(chunks), len(chunk_metadata)))\n",
        "     print(\"CRITICAL ERROR: Number of chunks and metadata entries do not match!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDI_SDwh-GUL",
        "outputId": "418afc40-8731-4cdf-b44d-f6c69fb81677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model loaded/re-loaded in 1.66 seconds.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Initialize Embedding Model (Re-run this cell)\n",
        "\n",
        "logging.info(\"Re-initializing the sentence embedding model...\") # Changed log message slightly\n",
        "\n",
        "# --- Configuration ---\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "# --- ---\n",
        "\n",
        "# Determine device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    logging.info(\"Detected CUDA-enabled GPU. Will use GPU for embeddings.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    logging.info(\"No CUDA-enabled GPU detected. Using CPU for embeddings.\")\n",
        "\n",
        "# Load the Sentence Transformer model\n",
        "try:\n",
        "    start_load_time = time.time()\n",
        "    # Instantiate the model, sending it to the chosen device\n",
        "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
        "    end_load_time = time.time()\n",
        "\n",
        "    # Confirm successful loading\n",
        "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully to '{device}'.\")\n",
        "    print(f\"Embedding model loaded/re-loaded in {end_load_time - start_load_time:.2f} seconds.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load embedding model '{EMBEDDING_MODEL_NAME}': {e}\", exc_info=True)\n",
        "    print(f\"Error: Could not load the embedding model. Please check the model name ('{EMBEDDING_MODEL_NAME}'), your internet connection, and library installations.\")\n",
        "    # raise e # Halt execution if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "3e286dfd4229497790d595b398c9bb56",
            "75ab9ae586e441248cf7f7f21f02c76f",
            "49d913f2c0e749b4b1e622191ebade45",
            "0f1549db1eae4f2e80d61e89e743b125",
            "135073f44ff7426e9572f0dcb07b2a5c",
            "2acd7c333c9c49589ea63821899e67a9",
            "29cd3c8fcef0467b8c29460745aba70c",
            "42eb6218a7e048bc9abdcd034ffcc04d",
            "8a8d891e94aa41b491e50d1423a84372",
            "e6e47248c56446a8a21b512639cc1927",
            "d7d71ae9ca424107aa28b0c2ef8d771e"
          ]
        },
        "id": "zHRQB9ag-3rB",
        "outputId": "3c65f936-baa5-42d2-811a-5c058c73d3ae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e286dfd4229497790d595b398c9bb56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embeddings generated for 499 chunks.\n",
            "Shape of embeddings array: (499, 384)\n",
            "Embeddings dimension (384D) matches model's expected dimension.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Generate Embeddings\n",
        "\n",
        "logging.info(\"Generating embeddings for the text chunks...\")\n",
        "\n",
        "# Ensure the 'chunks' list from Step 3 exists and is not empty\n",
        "if 'chunks' not in locals() or not chunks:\n",
        "    logging.error(\"Chunk list ('chunks') is missing or empty. Cannot generate embeddings. Please re-run Step 3.\")\n",
        "    print(\"Error: Text chunks not found. Please ensure Step 3 (Data Preprocessing and Chunking) ran successfully.\")\n",
        "    # Optionally, raise an error to halt execution if chunks are missing\n",
        "    # raise ValueError(\"Cannot generate embeddings without text chunks.\")\n",
        "else:\n",
        "    try:\n",
        "        logging.info(f\"Starting embedding generation for {len(chunks)} chunks using '{EMBEDDING_MODEL_NAME}' on device '{device}'...\")\n",
        "        start_embed_time = time.time()\n",
        "\n",
        "        # Use the encode method of the SentenceTransformer model\n",
        "        # convert_to_numpy=True ensures the output is a NumPy array suitable for FAISS\n",
        "        # show_progress_bar=True provides visual feedback during the process\n",
        "        chunk_embeddings = embedding_model.encode(\n",
        "            chunks,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        end_embed_time = time.time()\n",
        "\n",
        "        # --- Verification ---\n",
        "        # Get the expected dimension size from the model\n",
        "        embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
        "        actual_shape = chunk_embeddings.shape\n",
        "\n",
        "        logging.info(f\"Embeddings generated successfully in {end_embed_time - start_embed_time:.2f} seconds.\")\n",
        "        print(f\"\\nEmbeddings generated for {actual_shape[0]} chunks.\")\n",
        "        print(f\"Shape of embeddings array: {actual_shape}\") # Expected: (number_of_chunks, embedding_dimension)\n",
        "\n",
        "        # Verify dimensions match expectations\n",
        "        if actual_shape[0] == len(chunks) and actual_shape[1] == embedding_dimension:\n",
        "            print(f\"Embeddings dimension ({actual_shape[1]}D) matches model's expected dimension.\")\n",
        "        else:\n",
        "            logging.warning(f\"Embeddings shape mismatch! Expected ({len(chunks)}, {embedding_dimension}), Got {actual_shape}\")\n",
        "            print(f\"Warning: Embeddings array shape {actual_shape} differs from expected ({len(chunks)}, {embedding_dimension}).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during embedding generation: {e}\", exc_info=True)\n",
        "        print(f\"An error occurred while generating embeddings: {e}\")\n",
        "        # Depending on the error, might need to handle or halt\n",
        "        # raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_TjP5Zc_Mbm",
        "outputId": "6ea590cb-6eab-4cd1-9b03-6c4ca3e58690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FAISS index built successfully with 499 vectors.\n",
            "Index is trained: True\n",
            "\n",
            "Index and associated data saved:\n",
            "- Index: my_scheme_faiss.index\n",
            "- Metadata: my_scheme_metadata.json\n",
            "- Chunks: my_scheme_chunks.json\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Build and Save FAISS Index\n",
        "\n",
        "logging.info(\"Building the FAISS index...\")\n",
        "\n",
        "# Ensure the embeddings NumPy array exists from Step 5\n",
        "if 'chunk_embeddings' not in locals() or not isinstance(chunk_embeddings, np.ndarray):\n",
        "    logging.error(\"Embeddings array ('chunk_embeddings') not found. Cannot build index. Please re-run Step 5.\")\n",
        "    print(\"Error: Embeddings variable is missing. Please ensure Step 5 ran correctly.\")\n",
        "    # Optionally raise error\n",
        "    # raise NameError(\"chunk_embeddings not defined\")\n",
        "else:\n",
        "    try:\n",
        "        # 1. Get the dimensionality (d) from the embeddings array\n",
        "        d = chunk_embeddings.shape[1]\n",
        "        n_vectors = chunk_embeddings.shape[0]\n",
        "        logging.info(f\"Embeddings shape: ({n_vectors}, {d})\")\n",
        "\n",
        "        # 2. Normalize embeddings for IndexFlatIP (to compute Cosine Similarity)\n",
        "        # FAISS works with float32\n",
        "        if chunk_embeddings.dtype != np.float32:\n",
        "            logging.info(f\"Converting embeddings dtype from {chunk_embeddings.dtype} to float32.\")\n",
        "            chunk_embeddings = chunk_embeddings.astype(np.float32)\n",
        "\n",
        "        logging.info(\"Normalizing embeddings using faiss.normalize_L2...\")\n",
        "        faiss.normalize_L2(chunk_embeddings)\n",
        "        logging.info(\"Embeddings normalized successfully.\")\n",
        "\n",
        "        # 3. Create a FAISS index - IndexFlatIP supports Inner Product search\n",
        "        # For normalized vectors, max Inner Product corresponds to max Cosine Similarity\n",
        "        index = faiss.IndexFlatIP(d)\n",
        "        logging.info(f\"FAISS index created: Type=IndexFlatIP, Dimension={d}\")\n",
        "\n",
        "        # 4. Add the normalized vectors to the index\n",
        "        index.add(chunk_embeddings)\n",
        "        logging.info(f\"Added {index.ntotal} vectors to the index.\")\n",
        "\n",
        "        # --- Verification ---\n",
        "        if index.ntotal == n_vectors:\n",
        "            print(f\"\\nFAISS index built successfully with {index.ntotal} vectors.\")\n",
        "            print(f\"Index is trained: {index.is_trained}\") # IndexFlatIP requires no training\n",
        "        else:\n",
        "            logging.error(f\"Index count mismatch! Expected {n_vectors}, found {index.ntotal}\")\n",
        "            print(f\"\\nError: Number of vectors in index ({index.ntotal}) does not match input ({n_vectors}).\")\n",
        "\n",
        "\n",
        "        # --- Save Index and Associated Data ---\n",
        "        # Define filenames for persistence\n",
        "        FAISS_INDEX_FILE = \"my_scheme_faiss.index\"\n",
        "        METADATA_FILE = \"my_scheme_metadata.json\"\n",
        "        CHUNKS_FILE = \"my_scheme_chunks.json\"\n",
        "\n",
        "        # Save the FAISS index\n",
        "        logging.info(f\"Saving FAISS index to '{FAISS_INDEX_FILE}'...\")\n",
        "        faiss.write_index(index, FAISS_INDEX_FILE)\n",
        "        logging.info(\"FAISS index saved.\")\n",
        "\n",
        "        # Save the metadata (list of dictionaries)\n",
        "        logging.info(f\"Saving chunk metadata to '{METADATA_FILE}'...\")\n",
        "        # Ensure 'chunk_metadata' list exists from Step 3\n",
        "        if 'chunk_metadata' in locals() and chunk_metadata:\n",
        "            with open(METADATA_FILE, 'w', encoding='utf-8') as f_meta:\n",
        "                # Use json module to dump the list of dicts\n",
        "                json.dump(chunk_metadata, f_meta, ensure_ascii=False, indent=2)\n",
        "            logging.info(\"Chunk metadata saved.\")\n",
        "        else:\n",
        "            logging.warning(\"Chunk metadata not found or empty, cannot save metadata file.\")\n",
        "\n",
        "        # Save the actual text chunks (list of strings)\n",
        "        logging.info(f\"Saving text chunks to '{CHUNKS_FILE}'...\")\n",
        "        # Ensure 'chunks' list exists from Step 3\n",
        "        if 'chunks' in locals() and chunks:\n",
        "             with open(CHUNKS_FILE, 'w', encoding='utf-8') as f_chunks:\n",
        "                  json.dump(chunks, f_chunks, ensure_ascii=False, indent=2) # Save list of strings\n",
        "             logging.info(\"Text chunks saved.\")\n",
        "        else:\n",
        "             logging.warning(\"Chunks list not found or empty, cannot save chunks file.\")\n",
        "\n",
        "\n",
        "        print(f\"\\nIndex and associated data saved:\")\n",
        "        print(f\"- Index: {FAISS_INDEX_FILE}\")\n",
        "        print(f\"- Metadata: {METADATA_FILE}\")\n",
        "        print(f\"- Chunks: {CHUNKS_FILE}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during FAISS index building or saving: {e}\", exc_info=True)\n",
        "        print(f\"An error occurred during index creation/saving: {e}\")\n",
        "        # raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kla9zhNg_ah7",
        "outputId": "de91508f-5a58-43be-f156-fefe287db36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generator model loaded/re-loaded in 1.03 seconds.\n",
            "Model is actually on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Initialize Generator Model (Re-run this cell)\n",
        "\n",
        "logging.info(\"Re-initializing the Generator (Seq2Seq) Language Model...\") # Modified log message\n",
        "\n",
        "# --- Configuration ---\n",
        "GENERATOR_MODEL_NAME = \"google/flan-t5-base\"\n",
        "# --- ---\n",
        "\n",
        "# Re-determine device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    logging.info(f\"GPU ({torch.cuda.get_device_name(0)}) found. Attempting to load generator model to GPU.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    logging.info(\"Using CPU for generator model.\")\n",
        "\n",
        "try:\n",
        "    start_load_time = time.time()\n",
        "\n",
        "    # Load the tokenizer\n",
        "    generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME)\n",
        "    logging.info(f\"Tokenizer for '{GENERATOR_MODEL_NAME}' loaded.\")\n",
        "\n",
        "    # Load the model\n",
        "    try:\n",
        "         generator_model = AutoModelForSeq2SeqLM.from_pretrained(GENERATOR_MODEL_NAME, device_map=\"auto\")\n",
        "         logging.info(\"Generator model loaded with device_map='auto'.\")\n",
        "    except Exception as auto_map_error:\n",
        "         logging.warning(f\"Loading with device_map='auto' failed ({auto_map_error}). Attempting manual placement on '{device}'...\")\n",
        "         generator_model = AutoModelForSeq2SeqLM.from_pretrained(GENERATOR_MODEL_NAME)\n",
        "         generator_model.to(device) # Move the model to the determined device\n",
        "\n",
        "    end_load_time = time.time()\n",
        "\n",
        "    final_device = next(generator_model.parameters()).device\n",
        "    logging.info(f\"Generator model '{GENERATOR_MODEL_NAME}' is ready on device: {final_device}.\")\n",
        "    print(f\"\\nGenerator model loaded/re-loaded in {end_load_time - start_load_time:.2f} seconds.\")\n",
        "    print(f\"Model is actually on device: {final_device}\")\n",
        "\n",
        "    generator_model.eval()\n",
        "    logging.info(\"Generator model set to evaluation mode.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load generator model or tokenizer '{GENERATOR_MODEL_NAME}': {e}\", exc_info=True)\n",
        "    print(f\"Error: Could not load the generator model/tokenizer. Check model name, connection, and resources (RAM/VRAM).\")\n",
        "    # raise e # Halt execution if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-xs73Dx_tfn",
        "outputId": "f7a22be4-ceeb-4128-ce82-96e3d419cc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index loaded (Vectors: 499).\n",
            "Metadata loaded (499 entries).\n",
            "Chunks loaded (499 entries).\n",
            "Loaded data counts verified.\n",
            "\n",
            "Index, metadata, and chunks loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Cell 8a: Load Index and Data from Files\n",
        "\n",
        "logging.info(\"Attempting to load FAISS index, chunks, and metadata from files...\")\n",
        "\n",
        "# --- Configuration (Filenames from Step 6) ---\n",
        "FAISS_INDEX_FILE = \"my_scheme_faiss.index\"\n",
        "METADATA_FILE = \"my_scheme_metadata.json\"\n",
        "CHUNKS_FILE = \"my_scheme_chunks.json\"\n",
        "# --- ---\n",
        "\n",
        "# Flag to track loading success\n",
        "loaded_successfully = True\n",
        "\n",
        "try:\n",
        "    # Load FAISS index\n",
        "    logging.info(f\"Loading FAISS index from {FAISS_INDEX_FILE}...\")\n",
        "    index = faiss.read_index(FAISS_INDEX_FILE)\n",
        "    logging.info(f\"FAISS index loaded successfully with {index.ntotal} vectors.\")\n",
        "    print(f\"FAISS index loaded (Vectors: {index.ntotal}).\")\n",
        "\n",
        "    # Load metadata\n",
        "    logging.info(f\"Loading chunk metadata from {METADATA_FILE}...\")\n",
        "    with open(METADATA_FILE, 'r', encoding='utf-8') as f_meta:\n",
        "        chunk_metadata = json.load(f_meta)\n",
        "    logging.info(f\"Chunk metadata loaded successfully ({len(chunk_metadata)} entries).\")\n",
        "    print(f\"Metadata loaded ({len(chunk_metadata)} entries).\")\n",
        "\n",
        "    # Load chunks\n",
        "    logging.info(f\"Loading text chunks from {CHUNKS_FILE}...\")\n",
        "    with open(CHUNKS_FILE, 'r', encoding='utf-8') as f_chunks:\n",
        "        chunks = json.load(f_chunks)\n",
        "    logging.info(f\"Text chunks loaded successfully ({len(chunks)} entries).\")\n",
        "    print(f\"Chunks loaded ({len(chunks)} entries).\")\n",
        "\n",
        "    # Verify counts match\n",
        "    if not (index.ntotal == len(chunk_metadata) == len(chunks)):\n",
        "         logging.warning(\"Mismatch in loaded counts! Index: {}, Metadata: {}, Chunks: {}\".format(\n",
        "              index.ntotal, len(chunk_metadata), len(chunks)\n",
        "         ))\n",
        "         print(\"Warning: Counts of loaded items (index vectors, metadata, chunks) do not match!\")\n",
        "         # Decide if this is critical - might still work but indicates an issue.\n",
        "    else:\n",
        "         print(\"Loaded data counts verified.\")\n",
        "\n",
        "    # Check if embedding_model is loaded (should be from Cell 4)\n",
        "    if 'embedding_model' not in locals():\n",
        "         logging.error(\"Embedding model ('embedding_model') not found in environment.\")\n",
        "         print(\"Error: Embedding model is not loaded. Please re-run Cell 4.\")\n",
        "         loaded_successfully = False # Mark as failed\n",
        "         # raise NameError(\"embedding_model not loaded\") # Halt if critical\n",
        "\n",
        "    # Check if generator_model is loaded (should be from Cell 7)\n",
        "    if 'generator_model' not in locals() or 'generator_tokenizer' not in locals():\n",
        "         logging.warning(\"Generator model/tokenizer not found in environment. Will need Cell 7 to be run before generation.\")\n",
        "         # Don't mark as failed yet, only needed for generation step later\n",
        "         print(\"Note: Generator model/tokenizer not loaded (needed for Step 9/10).\")\n",
        "\n",
        "\n",
        "except FileNotFoundError as fnf_error:\n",
        "     logging.error(f\"Error loading files: {fnf_error}. Cannot proceed.\")\n",
        "     print(f\"\\nError: One or more required files not found ({FAISS_INDEX_FILE}, {METADATA_FILE}, {CHUNKS_FILE}).\")\n",
        "     print(\"Please ensure Step 6 (Build and Save FAISS Index) ran successfully and saved the files in the current directory.\")\n",
        "     loaded_successfully = False # Mark as failed\n",
        "     # raise # Stop execution\n",
        "except Exception as e:\n",
        "     logging.error(f\"An error occurred loading index/data from files: {e}\", exc_info=True)\n",
        "     print(f\"\\nAn error occurred while loading saved index/data: {e}\")\n",
        "     loaded_successfully = False # Mark as failed\n",
        "     # raise\n",
        "\n",
        "if loaded_successfully:\n",
        "    print(\"\\nIndex, metadata, and chunks loaded successfully.\")\n",
        "else:\n",
        "    print(\"\\nLoading failed. Cannot proceed with retrieval test reliably.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMXogsLQ_t4K",
        "outputId": "1f979c02-99ca-47f3-ad27-337a38bea507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Checking Variable Existence Before Test ==========\n",
            "Variable 'embedding_model': FOUND in locals(). Type: <class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
            "Variable 'index': FOUND in locals(). Type: <class 'faiss.swigfaiss_avx512.IndexFlatIP'>\n",
            "Variable 'chunks': FOUND in locals(). Type: <class 'list'>\n",
            "Variable 'chunk_metadata': FOUND in locals(). Type: <class 'list'>\n",
            "\n",
            "Overall check result: are_all_vars_defined = True\n",
            "==================================================\n",
            "\n",
            "--------------------------------------------------\n",
            "--- Testing Retrieval Function ---\n",
            "Test Query: \"What schemes are available for farmers in Maharashtra?\"\n",
            "\n",
            "Successfully retrieved 3 results:\n",
            "\n",
            "--- Result #1 ---\n",
            "  Similarity Score: 0.7096\n",
            "  Metadata:\n",
            "    scheme_name: YSR Jala Kala\n",
            "    source_url: https://www.myscheme.gov.in/schemes/ysrjk\n",
            "    field: Eligibility\n",
            "    original_index: 97\n",
            "  Chunk Text (Snippet):\n",
            "    \"Scheme: YSR Jala Kala | Eligibility: The applicant should be a Farmer. The applicant should be a resident of the state of Andhra Pradesh. The applicant should be without an existing bore well/tube well OR having a failed bore well/tube well....\"\n",
            "\n",
            "--- Result #2 ---\n",
            "  Similarity Score: 0.7046\n",
            "  Metadata:\n",
            "    scheme_name: Atma Nirbhar Krishi Yojana\n",
            "    source_url: https://www.myscheme.gov.in/schemes/anky\n",
            "    field: Benefits\n",
            "    original_index: 91\n",
            "  Chunk Text (Snippet):\n",
            "    \"Scheme: Atma Nirbhar Krishi Yojana | Benefits: Financial assistance for agricultural activities: The scheme provides financial support to farmers for activities like scientific land terracing, doubling cropping, farm mechanization, and apiculture. Loan without collateral: Farmers can avail loans up ...\"\n",
            "\n",
            "--- Result #3 ---\n",
            "  Similarity Score: 0.6960\n",
            "  Metadata:\n",
            "    scheme_name: Krushy Aranya Protsaha Yojane (kapy)\n",
            "    source_url: https://www.myscheme.gov.in/schemes/kapy\n",
            "    field: Eligibility\n",
            "    original_index: 92\n",
            "  Chunk Text (Snippet):\n",
            "    \"Scheme: Krushy Aranya Protsaha Yojane (kapy) | Eligibility: This scheme is open to farmers belonging to all communities. Applicant must have Pahani of the land where planting is being proposed. Registration should be done before the commencement of the rainy season (by the end of May). The following...\"\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Cell 8 (Explicit Debug Check): Define Retrieval Function\n",
        "\n",
        "logging.info(\"Defining the retrieval function...\")\n",
        "\n",
        "# --- Function Definition (same as before) ---\n",
        "def retrieve_relevant_chunks(query: str,\n",
        "                              model: SentenceTransformer,\n",
        "                              faiss_index: faiss.Index,\n",
        "                              chunk_list: list,\n",
        "                              metadata_list: list,\n",
        "                              top_k: int = 5):\n",
        "    if not query:\n",
        "        logging.warning(\"Received empty query for retrieval.\")\n",
        "        return []\n",
        "    logging.info(f\"Retrieving top {top_k} relevant chunks for query: \\\"{query[:100]}...\\\"\")\n",
        "    try:\n",
        "        t_embed_start = time.time()\n",
        "        query_embedding = model.encode([query], convert_to_numpy=True, show_progress_bar=False)\n",
        "        query_embedding = query_embedding.astype(np.float32)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        t_embed_end = time.time()\n",
        "        logging.debug(f\"Query embedding generated and normalized in {t_embed_end - t_embed_start:.4f} sec. Shape: {query_embedding.shape}\")\n",
        "        t_search_start = time.time()\n",
        "        scores, indices = faiss_index.search(query_embedding, top_k)\n",
        "        t_search_end = time.time()\n",
        "        logging.debug(f\"FAISS search completed in {t_search_end - t_search_start:.4f} sec.\")\n",
        "        results = []\n",
        "        if indices.size > 0 and scores.size > 0:\n",
        "            retrieved_indices = indices[0]\n",
        "            retrieved_scores = scores[0]\n",
        "            logging.debug(f\"Retrieved indices: {retrieved_indices}\")\n",
        "            logging.debug(f\"Retrieved scores: {retrieved_scores}\")\n",
        "            for i, idx in enumerate(retrieved_indices):\n",
        "                if idx != -1:\n",
        "                     if 0 <= idx < len(chunk_list) and 0 <= idx < len(metadata_list):\n",
        "                          results.append({\n",
        "                              \"chunk_text\": chunk_list[idx],\n",
        "                              \"metadata\": metadata_list[idx],\n",
        "                              \"score\": float(retrieved_scores[i])\n",
        "                          })\n",
        "                          logging.debug(f\"  -> Added chunk index {idx} (Score: {retrieved_scores[i]:.4f})\")\n",
        "                     else:\n",
        "                          logging.warning(f\"Retrieved index {idx} is out of bounds for chunk/metadata lists (Size: {len(chunk_list)}). Skipping.\")\n",
        "                else:\n",
        "                     logging.debug(f\"Retrieved index was -1 at position {i}.\")\n",
        "        logging.info(f\"Retrieved {len(results)} chunks for the query.\")\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during retrieval for query \\\"{query}\\\": {e}\", exc_info=True)\n",
        "        return []\n",
        "# --- End of Function Definition ---\n",
        "\n",
        "\n",
        "# --- Explicit Debugging Check ---\n",
        "print(\"\\n\" + \"=\"*10 + \" Checking Variable Existence Before Test \" + \"=\"*10)\n",
        "required_vars_to_check = ['embedding_model', 'index', 'chunks', 'chunk_metadata']\n",
        "are_all_vars_defined = True\n",
        "variables_status = {}\n",
        "\n",
        "# Check both local and global scopes explicitly\n",
        "current_locals = locals()\n",
        "current_globals = globals()\n",
        "\n",
        "for var_name in required_vars_to_check:\n",
        "    found_in_locals = var_name in current_locals\n",
        "    found_in_globals = var_name in current_globals\n",
        "\n",
        "    if found_in_locals:\n",
        "        status = f\"FOUND in locals(). Type: {type(current_locals[var_name])}\"\n",
        "        variables_status[var_name] = True\n",
        "    elif found_in_globals:\n",
        "        status = f\"FOUND in globals(). Type: {type(current_globals[var_name])}\"\n",
        "        variables_status[var_name] = True\n",
        "    else:\n",
        "        status = \"NOT FOUND in locals() or globals().\"\n",
        "        variables_status[var_name] = False\n",
        "        are_all_vars_defined = False # Mark that at least one is missing\n",
        "\n",
        "    print(f\"Variable '{var_name}': {status}\")\n",
        "\n",
        "print(f\"\\nOverall check result: are_all_vars_defined = {are_all_vars_defined}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "# --- End Explicit Debugging Check ---\n",
        "\n",
        "\n",
        "# --- Quick Test of the Retrieval Function ---\n",
        "# Use the flag determined by the explicit check above\n",
        "if are_all_vars_defined:\n",
        "    print(\"-\" * 50)\n",
        "    print(\"--- Testing Retrieval Function ---\")\n",
        "    test_query = \"What schemes are available for farmers in Maharashtra?\"\n",
        "    print(f\"Test Query: \\\"{test_query}\\\"\")\n",
        "    # Directly use the variables, assuming the check passed\n",
        "    retrieved_data = retrieve_relevant_chunks(test_query, embedding_model, index, chunks, chunk_metadata, top_k=3)\n",
        "\n",
        "    if retrieved_data:\n",
        "        print(f\"\\nSuccessfully retrieved {len(retrieved_data)} results:\")\n",
        "        for rank, result in enumerate(retrieved_data, 1):\n",
        "            print(f\"\\n--- Result #{rank} ---\")\n",
        "            print(f\"  Similarity Score: {result['score']:.4f}\")\n",
        "            print(f\"  Metadata:\")\n",
        "            for k, v in result['metadata'].items():\n",
        "                 print(f\"    {k}: {v}\")\n",
        "            print(f\"  Chunk Text (Snippet):\")\n",
        "            print(f\"    \\\"{result['chunk_text'][:300]}...\\\"\")\n",
        "    else:\n",
        "        print(\"\\nNo results were retrieved for the test query.\")\n",
        "    print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"\\nSkipping retrieval function test because the explicit check confirmed one or more required components were missing.\")\n",
        "    logging.warning(\"Skipping retrieval test because explicit components check failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZqEefKmBjyA",
        "outputId": "0482973c-bfea-4d10-e0ae-f88efc024742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "--- Testing Generation Function ---\n",
            "Test Generation Query: 'Who is eligible for YSR Jala Kala?'\n",
            "Using context from 3 chunks retrieved in the previous test.\n",
            "\n",
            "Generated Answer:\n",
            "A Farmer.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Cell 9 (Attempt 3 - Remove Pre-check): Define Generation Function\n",
        "\n",
        "logging.info(\"Defining the answer generation function (Attempt 3)...\")\n",
        "\n",
        "# Assuming generator_model and generator_tokenizer exist from Cell 7 execution\n",
        "\n",
        "# --- Generation Configuration ---\n",
        "MAX_ANSWER_LENGTH = 256\n",
        "NUM_BEAMS = 4\n",
        "NO_REPEAT_NGRAM = 2\n",
        "EARLY_STOPPING = True\n",
        "# --- ---\n",
        "\n",
        "def generate_answer(query: str,\n",
        "                     context_chunk_texts: list,\n",
        "                     model: AutoModelForSeq2SeqLM,\n",
        "                     tokenizer: AutoTokenizer):\n",
        "    \"\"\"\n",
        "    Generates an answer using the seq2seq model based on the query and context.\n",
        "    \"\"\"\n",
        "    # Try using the models directly - Python will raise NameError if they don't exist\n",
        "    # Use global keyword if needed, though generally not required in notebooks unless defining inside another function\n",
        "    # global generator_model, generator_tokenizer\n",
        "\n",
        "    if not context_chunk_texts:\n",
        "        logging.warning(\"generate_answer called with no context provided.\")\n",
        "        return \"Based on the available scheme information, I could not find specific details to answer your question.\"\n",
        "\n",
        "    context_string = \"\\n\\n\".join(context_chunk_texts)\n",
        "    prompt_template = \"\"\"Please answer the following question based *only* on the provided context information. If the context does not contain the answer, please state 'Based on the provided context, I cannot answer this question'.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    prompt = prompt_template.format(context=context_string, question=query)\n",
        "\n",
        "    logging.info(f\"Generating answer for query: '{query[:100]}...'\")\n",
        "    logging.debug(f\"Generator Prompt (start):\\n{prompt[:500]}...\")\n",
        "\n",
        "    try:\n",
        "        t_token_start = time.time()\n",
        "        # Use the assumed globally available 'generator_tokenizer' and 'generator_model'\n",
        "        inputs = generator_tokenizer(prompt,\n",
        "                                      return_tensors=\"pt\",\n",
        "                                      max_length=1024,\n",
        "                                      truncation=True\n",
        "                                     ).to(generator_model.device) # Use model's stored device\n",
        "        t_token_end = time.time()\n",
        "        logging.debug(f\"Prompt tokenized in {t_token_end - t_token_start:.4f} sec.\")\n",
        "\n",
        "        t_gen_start = time.time()\n",
        "        output_sequences = generator_model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=MAX_ANSWER_LENGTH,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            early_stopping=EARLY_STOPPING,\n",
        "            no_repeat_ngram_size=NO_REPEAT_NGRAM\n",
        "        )\n",
        "        t_gen_end = time.time()\n",
        "        logging.debug(f\"Output sequences generated in {t_gen_end - t_gen_start:.4f} sec.\")\n",
        "\n",
        "        t_decode_start = time.time()\n",
        "        generated_text = generator_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "        t_decode_end = time.time()\n",
        "        logging.debug(f\"Response decoded in {t_decode_end - t_decode_start:.4f} sec.\")\n",
        "\n",
        "        final_answer = generated_text.strip()\n",
        "        logging.info(f\"Generated Answer (final): '{final_answer[:150]}...'\")\n",
        "        return final_answer\n",
        "\n",
        "    except NameError as ne:\n",
        "         # Catch NameError specifically if model/tokenizer weren't actually loaded\n",
        "         logging.error(f\"NameError during generation: {ne}. Model or Tokenizer likely missing.\", exc_info=True)\n",
        "         print(f\"ERROR: A required model component ({ne}) was not found. Please ensure Cell 7 was run successfully.\")\n",
        "         return \"Error: Generation components missing.\"\n",
        "    except Exception as e:\n",
        "         logging.error(f\"An error occurred during answer generation: {e}\", exc_info=True)\n",
        "         return \"I encountered an error while trying to generate the answer.\"\n",
        "\n",
        "\n",
        "# --- Quick Test of the Generation Function ---\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"--- Testing Generation Function ---\")\n",
        "\n",
        "# Check for retrieved_data existence before testing\n",
        "if 'retrieved_data' in locals() and retrieved_data:\n",
        "    test_context_texts = [res['chunk_text'] for res in retrieved_data]\n",
        "    # Check if the test variables actually exist before calling generate_answer\n",
        "    if 'generator_model' in locals() and 'generator_tokenizer' in locals():\n",
        "        test_gen_query = \"Who is eligible for YSR Jala Kala?\"\n",
        "\n",
        "        print(f\"Test Generation Query: '{test_gen_query}'\")\n",
        "        print(f\"Using context from {len(test_context_texts)} chunks retrieved in the previous test.\")\n",
        "\n",
        "        # Call the generation function\n",
        "        generated_answer = generate_answer(test_gen_query, test_context_texts, generator_model, generator_tokenizer)\n",
        "\n",
        "        print(\"\\nGenerated Answer:\")\n",
        "        print(generated_answer)\n",
        "    else:\n",
        "        print(\"\\nCannot run generation test because 'generator_model' or 'generator_tokenizer' is missing.\")\n",
        "        logging.error(\"Cannot run generation test - generator model/tokenizer missing.\")\n",
        "\n",
        "else:\n",
        "    # This might happen if Cell 8's test failed or wasn't run\n",
        "    print(\"\\nSkipping generation test as no retrieved context ('retrieved_data') is available.\")\n",
        "    logging.warning(\"Skipping generation test - no context available.\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVf2A3KrDD_-",
        "outputId": "47c4668c-691c-46f6-b2e2-d612d0e69493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "--- Testing the Full QA System ---\n",
            "\n",
            "--- Query #1 ---\n",
            "Q: What schemes are available for farmers in Maharashtra?\n",
            "A: Chartered Accountants\n",
            "\n",
            "--- Query #2 ---\n",
            "Q: What is the benefit amount for the PM Scholarship for RPF?\n",
            "A: 2500/- per month for male students\n",
            "\n",
            "--- Query #3 ---\n",
            "Q: Tell me about eligibility for Snehasanthwanam.\n",
            "A: The applicant should be a resident of Kerala State\n",
            "\n",
            "--- Query #4 ---\n",
            "Q: Are there any schemes for building houses?\n",
            "A: The Atal Mission For Rejuvenation And Urban Transformation\n",
            "\n",
            "--- Query #5 ---\n",
            "Q: What are the documents required for the Laadli Laxmi Scheme?\n",
            "A: The application shall be submitted within one year, from the date of attaining the age of 18 years\n",
            "\n",
            "--- Query #6 ---\n",
            "Q: What is the MyScheme portal?\n",
            "A: Department of Education.\n",
            "\n",
            "==================================================\n",
            "--- QA System Testing Complete ---\n"
          ]
        }
      ],
      "source": [
        "# Cell 10 (Corrected NameError): Define Combined QA Function and Test\n",
        "\n",
        "logging.info(\"Defining the main QA function...\")\n",
        "\n",
        "# Check dependencies are loaded one last time before defining the main function\n",
        "required_components = [\n",
        "    'embedding_model', 'index', 'chunks', 'chunk_metadata',\n",
        "    'generator_model', 'generator_tokenizer',\n",
        "    'retrieve_relevant_chunks', 'generate_answer'\n",
        "]\n",
        "# Also check for LOGGING_LEVEL which we will use for the debug check\n",
        "if 'LOGGING_LEVEL' not in locals() and 'LOGGING_LEVEL' not in globals():\n",
        "     # If LOGGING_LEVEL wasn't set explicitly, check the root logger's level\n",
        "     if logging.getLogger().hasHandlers():\n",
        "          LOGGING_LEVEL = logging.getLogger().getEffectiveLevel()\n",
        "          logging.info(f\"LOGGING_LEVEL variable not found, inferred level as {logging.getLevelName(LOGGING_LEVEL)} from root logger.\")\n",
        "     else:\n",
        "          # Fallback if logging wasn't configured properly\n",
        "          LOGGING_LEVEL = logging.WARNING\n",
        "          logging.warning(\"LOGGING_LEVEL variable not found and root logger not configured, defaulting to WARNING.\")\n",
        "\n",
        "\n",
        "if not all(comp in locals() or comp in globals() for comp in required_components):\n",
        "     logging.critical(\"Cannot define QA function - one or more critical components are missing!\")\n",
        "     print(\"\\nError: Critical components missing. Cannot proceed. Please ensure previous cells ran successfully.\")\n",
        "     # raise NameError(\"Missing components for QA function\")\n",
        "else:\n",
        "     def ask_scheme_question(query: str, top_k_chunks: int = 5):\n",
        "          \"\"\"\n",
        "          Orchestrates the RAG process: retrieves context and generates an answer.\n",
        "          Args:\n",
        "              query (str): The user's question about government schemes.\n",
        "              top_k_chunks (int): The number of relevant chunks to retrieve.\n",
        "          Returns:\n",
        "              str: The generated answer.\n",
        "          \"\"\"\n",
        "          logging.info(f\"Processing QA query: \\\"{query}\\\"\")\n",
        "\n",
        "          # 1. Retrieve Relevant Context Chunks\n",
        "          retrieved_context = retrieve_relevant_chunks(\n",
        "              query=query,\n",
        "              model=embedding_model,\n",
        "              faiss_index=index,\n",
        "              chunk_list=chunks,\n",
        "              metadata_list=chunk_metadata,\n",
        "              top_k=top_k_chunks # Corrected keyword argument from previous step\n",
        "          )\n",
        "\n",
        "          if not retrieved_context:\n",
        "               logging.warning(\"Retrieval returned no relevant chunks for the query.\")\n",
        "               return \"I couldn't find specific information related to your question in the available scheme data.\"\n",
        "\n",
        "          context_texts = [item['chunk_text'] for item in retrieved_context]\n",
        "\n",
        "          # --- CORRECTED LINE: Use LOGGING_LEVEL ---\n",
        "          if LOGGING_LEVEL <= logging.DEBUG:\n",
        "               logging.debug(f\"Passing {len(context_texts)} chunks to the generator.\")\n",
        "               for i, txt in enumerate(context_texts):\n",
        "                    logging.debug(f\"Context Chunk {i+1}: {txt[:100]}...\")\n",
        "          # --- END CORRECTION ---\n",
        "\n",
        "          # 2. Generate Answer using Retrieved Context\n",
        "          final_answer = generate_answer(\n",
        "              query=query,\n",
        "              context_chunk_texts=context_texts,\n",
        "              model=generator_model,\n",
        "              tokenizer=generator_tokenizer\n",
        "          )\n",
        "\n",
        "          # 3. Return the generated answer\n",
        "          return final_answer\n",
        "\n",
        "     # --- Test the Full QA Pipeline ---\n",
        "     print(\"\\n\" + \"=\"*50)\n",
        "     print(\"--- Testing the Full QA System ---\")\n",
        "\n",
        "     test_queries = [\n",
        "         \"What schemes are available for farmers in Maharashtra?\", # User's original example\n",
        "         \"What is the benefit amount for the PM Scholarship for RPF?\", # Specific, likely answerable\n",
        "         \"Tell me about eligibility for Snehasanthwanam.\", # Specific, likely answerable\n",
        "         \"Are there any schemes for building houses?\", # General, might find relevant context\n",
        "         \"What are the documents required for the Laadli Laxmi Scheme?\", # Specific, likely answerable\n",
        "         \"What is the MyScheme portal?\" # General knowledge, unlikely in context\n",
        "     ]\n",
        "\n",
        "     for i, q in enumerate(test_queries, 1):\n",
        "          print(f\"\\n--- Query #{i} ---\")\n",
        "          print(f\"Q: {q}\")\n",
        "          answer = ask_scheme_question(q, top_k_chunks=5) # Use top 5 chunks\n",
        "          print(f\"A: {answer}\")\n",
        "          # time.sleep(1) # Optional delay\n",
        "\n",
        "     print(\"\\n\" + \"=\"*50)\n",
        "     print(\"--- QA System Testing Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vN3kvphERgt",
        "outputId": "2ee2c98d-4643-4a6b-a2d8-c505175e27e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Browser window (if any) likely already closed or was not used in this session.\n",
            "\n",
            "QA System setup and testing complete.\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Quit WebDriver\n",
        "\n",
        "try:\n",
        "    # Check if 'driver' variable exists and holds a WebDriver instance\n",
        "    if 'driver' in locals() and isinstance(driver, webdriver.WebDriver):\n",
        "        logging.info(\"Closing the Selenium WebDriver...\")\n",
        "        driver.quit()\n",
        "        logging.info(\"WebDriver closed successfully.\")\n",
        "        print(\"\\nBrowser window (if any) closed.\")\n",
        "        # Optional: remove variable from namespace\n",
        "        # del driver\n",
        "    elif 'driver' in locals():\n",
        "         logging.warning(\"Variable 'driver' exists but is not a WebDriver instance or might be None.\")\n",
        "         print(\"\\nWebDriver already closed or variable reassigned.\")\n",
        "    else:\n",
        "        logging.info(\"WebDriver variable 'driver' not found. Assuming already closed.\")\n",
        "        print(\"\\nBrowser window (if any) likely already closed or was not used in this session.\")\n",
        "except NameError:\n",
        "     logging.info(\"WebDriver variable 'driver' not defined in this scope.\")\n",
        "     print(\"\\nBrowser window (if any) likely already closed or was not used in this session.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"An error occurred while trying to close the WebDriver: {e}\")\n",
        "    print(f\"\\nAn error occurred closing browser: {e}\")\n",
        "\n",
        "print(\"\\nQA System setup and testing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M-EiEATGu5z",
        "outputId": "9345900e-061c-4f1d-8525-5cc8e605d1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "--- Testing the RAG QA System (using loaded index) ---\n",
            "\n",
            "--- Query #1 ---\n",
            "Q: What schemes are available for farmers in Maharashtra?\n",
            "A: ANKY\n",
            "\n",
            "--- Query #2 ---\n",
            "Q: What is the benefit amount for the PM Scholarship for RPF?\n",
            "A: Scholarships to Students with Disabilities\n",
            "\n",
            "--- Query #3 ---\n",
            "Q: Tell me about eligibility for Snehasanthwanam.\n",
            "A: The applicant should be a resident of Kerala State\n",
            "\n",
            "--- Query #4 ---\n",
            "Q: Are there any schemes for building houses?\n",
            "A: Storm Water Drainage\n",
            "\n",
            "--- Query #5 ---\n",
            "Q: What are the documents required for the Laadli Laxmi Scheme?\n",
            "A: Laadli Laxmi Scheme\n",
            "\n",
            "--- Query #6 ---\n",
            "Q: What is the MyScheme portal?\n",
            "A: A receipt should be sent to the concerned District.\n",
            "\n",
            "==================================================\n",
            "\n",
            "============================================================\n",
            "RAG QA Script Execution Summary:\n",
            "============================================================\n",
            "- RAG QA Phase: Completed (Index/Data loaded from files, tests run)\n",
            "\n",
            "Total Execution Time: 5.34 seconds.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#Complete code (combined all cell)\n",
        "# run_scheme_qa.py\n",
        "# RAG QA system using pre-processed MyScheme data.\n",
        "# Assumes scraping is done and necessary RAG data files exist.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import json\n",
        "import os # For checking file existence\n",
        "\n",
        "# RAG Imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# --- Configuration ---\n",
        "# Input Files (Assumed to exist from previous indexing step)\n",
        "FAISS_INDEX_FILE = \"my_scheme_faiss.index\"\n",
        "METADATA_FILE = \"my_scheme_metadata.json\"\n",
        "CHUNKS_FILE = \"my_scheme_chunks.json\"\n",
        "\n",
        "# Model Config\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "GENERATOR_MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "# RAG Config\n",
        "RAG_TOP_K = 5 # Number of chunks to retrieve for RAG context\n",
        "\n",
        "# General Config\n",
        "LOGGING_LEVEL = logging.INFO # Change to logging.DEBUG for very detailed output\n",
        "\n",
        "# Configure Logging\n",
        "logging.basicConfig(level=LOGGING_LEVEL,\n",
        "                    format='%(asctime)s [%(levelname)-8s] %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# --- RAG QA SYSTEM ---\n",
        "\n",
        "def setup_and_run_qa_system(index_path=FAISS_INDEX_FILE,\n",
        "                             metadata_path=METADATA_FILE,\n",
        "                             chunks_path=CHUNKS_FILE):\n",
        "    \"\"\"\n",
        "    Loads pre-built RAG components (index, chunks, metadata),\n",
        "    loads necessary models (embedding, generator), defines QA functions,\n",
        "    and runs test queries.\n",
        "    \"\"\"\n",
        "    log.info(\"--- Starting RAG QA System Setup & Test ---\")\n",
        "    start_rag_time = time.time()\n",
        "\n",
        "    # --- 1. Load Pre-built Index and Data ---\n",
        "    log.info(\"Loading pre-built FAISS index and associated data...\")\n",
        "    required_files = [index_path, metadata_path, chunks_path]\n",
        "    if not all(os.path.exists(f) for f in required_files):\n",
        "        log.critical(f\"One or more required RAG data files not found: {required_files}. Cannot proceed.\")\n",
        "        print(f\"Error: Missing required input files. Please ensure '{index_path}', '{metadata_path}', and '{chunks_path}' exist from the previous processing step.\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    try:\n",
        "        index = faiss.read_index(index_path)\n",
        "        log.info(f\"FAISS index loaded from '{index_path}' ({index.ntotal} vectors).\")\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            chunk_metadata = json.load(f)\n",
        "        log.info(f\"Chunk metadata loaded from '{metadata_path}' ({len(chunk_metadata)} entries).\")\n",
        "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
        "            chunks = json.load(f)\n",
        "        log.info(f\"Text chunks loaded from '{chunks_path}' ({len(chunks)} entries).\")\n",
        "\n",
        "        # Verify counts match\n",
        "        if not (index.ntotal == len(chunk_metadata) == len(chunks)):\n",
        "             log.warning(f\"Loaded data count mismatch! Index:{index.ntotal}, Meta:{len(chunk_metadata)}, Chunks:{len(chunks)}\")\n",
        "             print(\"Warning: Counts of loaded items (index vectors, metadata, chunks) do not match! Proceeding cautiously.\")\n",
        "             # Decide if this is critical based on use case\n",
        "    except Exception as e:\n",
        "        log.critical(f\"Failed to load RAG data files: {e}\", exc_info=True)\n",
        "        print(f\"Error loading required data files: {e}\")\n",
        "        return False\n",
        "\n",
        "    # --- 2. Load Embedding Model (Needed for querying) ---\n",
        "    log.info(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    log.info(f\"Using device: {device} for embeddings\")\n",
        "    try:\n",
        "        # Define embedding_model in this scope\n",
        "        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
        "        log.info(\"Embedding model loaded.\")\n",
        "    except Exception as e:\n",
        "        log.critical(f\"Failed to load embedding model: {e}\", exc_info=True)\n",
        "        print(f\"Error loading embedding model: {e}\")\n",
        "        return False\n",
        "\n",
        "    # --- 3. Load Generator Model (Needed for generation) ---\n",
        "    log.info(f\"Loading generator model: {GENERATOR_MODEL_NAME}...\")\n",
        "    try:\n",
        "        # Define generator_tokenizer and generator_model in this scope\n",
        "        generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME)\n",
        "        # Use device_map='auto' for flexible placement (GPU/CPU/multi-GPU)\n",
        "        generator_model = AutoModelForSeq2SeqLM.from_pretrained(GENERATOR_MODEL_NAME, device_map=\"auto\")\n",
        "        generator_model.eval() # Set to evaluation mode\n",
        "        log.info(f\"Generator model ready on device(s): {generator_model.device_map if hasattr(generator_model, 'device_map') else next(generator_model.parameters()).device}\")\n",
        "    except Exception as e:\n",
        "        log.critical(f\"Failed to load generator model/tokenizer: {e}\", exc_info=True)\n",
        "        print(f\"Error loading generator model: {e}\")\n",
        "        return False\n",
        "\n",
        "    # --- 4. Define RAG Functions ---\n",
        "    log.info(\"Defining RAG pipeline functions...\")\n",
        "\n",
        "    # Retrieval Function\n",
        "    def retrieve_relevant_chunks_rag(query: str, top_k: int = RAG_TOP_K):\n",
        "        # This function now uses the variables loaded/defined within setup_and_run_qa_system\n",
        "        logging.debug(f\"Retrieving top {top_k} for: '{query[:60]}...'\")\n",
        "        try:\n",
        "            query_embedding = embedding_model.encode([query], convert_to_numpy=True, show_progress_bar=False)\n",
        "            query_embedding = query_embedding.astype(np.float32)\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "            scores, indices = index.search(query_embedding, top_k)\n",
        "            results = []\n",
        "            if indices.size > 0:\n",
        "                 for i, idx in enumerate(indices[0]):\n",
        "                      if idx != -1 and 0 <= idx < len(chunks):\n",
        "                           results.append({\"chunk_text\": chunks[idx], \"metadata\": chunk_metadata[idx], \"score\": float(scores[0][i])})\n",
        "            results.sort(key=lambda x: x['score'], reverse=True) # Highest score first\n",
        "            log.debug(f\"Retrieved {len(results)} chunks.\")\n",
        "            return results\n",
        "        except Exception as e: log.error(f\"Retrieval error: {e}\", exc_info=True); return []\n",
        "\n",
        "    # Generation Function\n",
        "    def generate_answer_rag(query: str, context_chunk_texts: list):\n",
        "        if not context_chunk_texts: return \"I couldn't find relevant information in the provided context.\"\n",
        "        context_string = \"\\n\\n\".join(context_chunk_texts)\n",
        "        prompt_template = \"\"\"Please answer the following question based *only* on the provided context information. If the context does not contain the answer, please state 'Based on the provided context, I cannot answer this question'.\n",
        "\n",
        "Context:\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        prompt = prompt_template.format(context=context_string, question=query)\n",
        "        log.debug(f\"Generator prompt (start): {prompt[:300]}...\")\n",
        "        try:\n",
        "            inputs = generator_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(generator_model.device)\n",
        "            outputs = generator_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256, # Max tokens for the generated answer\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2\n",
        "                )\n",
        "            answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            log.debug(f\"Generated answer: {answer[:100]}...\")\n",
        "            return answer.strip()\n",
        "        except Exception as e: log.error(f\"Generation error: {e}\", exc_info=True); return \"Error during answer generation.\"\n",
        "\n",
        "    # Combined QA Function\n",
        "    def ask_scheme_question_rag(query: str):\n",
        "        log.info(f\"QA Query: \\\"{query}\\\"\")\n",
        "        retrieved = retrieve_relevant_chunks_rag(query, top_k=RAG_TOP_K)\n",
        "        if not retrieved:\n",
        "            log.warning(\"No relevant chunks retrieved.\")\n",
        "            return \"I couldn't find specific information related to your question in the available scheme data.\"\n",
        "\n",
        "        context = [r['chunk_text'] for r in retrieved]\n",
        "        answer = generate_answer_rag(query, context)\n",
        "        return answer\n",
        "\n",
        "    # --- 5. Test QA System ---\n",
        "    log.info(\"--- Testing QA System ---\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- Testing the RAG QA System (using loaded index) ---\")\n",
        "    test_queries = [\n",
        "         \"What schemes are available for farmers in Maharashtra?\",\n",
        "         \"What is the benefit amount for the PM Scholarship for RPF?\",\n",
        "         \"Tell me about eligibility for Snehasanthwanam.\",\n",
        "         \"Are there any schemes for building houses?\",\n",
        "         \"What are the documents required for the Laadli Laxmi Scheme?\",\n",
        "         \"What is the MyScheme portal?\" # Expected to fail gracefully\n",
        "     ]\n",
        "    for i, q in enumerate(test_queries):\n",
        "         print(f\"\\n--- Query #{i+1} ---\")\n",
        "         print(f\"Q: {q}\")\n",
        "         answer = ask_scheme_question_rag(q)\n",
        "         print(f\"A: {answer}\") # Also print answer to console\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    log.info(f\"--- RAG QA Setup & Test Finished. Duration: {time.time() - start_rag_time:.2f} sec ---\")\n",
        "    return True # Indicate success\n",
        "\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "if __name__ == \"__main__\":\n",
        "    overall_start_time = time.time()\n",
        "    log.info(\"Starting RAG QA script (using pre-built index)...\")\n",
        "\n",
        "    # Directly run the RAG setup and testing function\n",
        "    rag_success = setup_and_run_qa_system()\n",
        "\n",
        "    # Final Summary\n",
        "    overall_end_time = time.time()\n",
        "    log.info(\"RAG QA script finished.\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RAG QA Script Execution Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    if rag_success:\n",
        "        print(f\"- RAG QA Phase: Completed (Index/Data loaded from files, tests run)\")\n",
        "    else:\n",
        "        print(\"- RAG QA Phase: Failed or Did Not Complete (Check logs for errors)\")\n",
        "\n",
        "    print(f\"\\nTotal Execution Time: {overall_end_time - overall_start_time:.2f} seconds.\")\n",
        "    print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f1549db1eae4f2e80d61e89e743b125": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e47248c56446a8a21b512639cc1927",
            "placeholder": "​",
            "style": "IPY_MODEL_d7d71ae9ca424107aa28b0c2ef8d771e",
            "value": " 16/16 [00:02&lt;00:00, 14.39it/s]"
          }
        },
        "135073f44ff7426e9572f0dcb07b2a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29cd3c8fcef0467b8c29460745aba70c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2acd7c333c9c49589ea63821899e67a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e286dfd4229497790d595b398c9bb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75ab9ae586e441248cf7f7f21f02c76f",
              "IPY_MODEL_49d913f2c0e749b4b1e622191ebade45",
              "IPY_MODEL_0f1549db1eae4f2e80d61e89e743b125"
            ],
            "layout": "IPY_MODEL_135073f44ff7426e9572f0dcb07b2a5c"
          }
        },
        "42eb6218a7e048bc9abdcd034ffcc04d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d913f2c0e749b4b1e622191ebade45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42eb6218a7e048bc9abdcd034ffcc04d",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a8d891e94aa41b491e50d1423a84372",
            "value": 16
          }
        },
        "75ab9ae586e441248cf7f7f21f02c76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2acd7c333c9c49589ea63821899e67a9",
            "placeholder": "​",
            "style": "IPY_MODEL_29cd3c8fcef0467b8c29460745aba70c",
            "value": "Batches: 100%"
          }
        },
        "8a8d891e94aa41b491e50d1423a84372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7d71ae9ca424107aa28b0c2ef8d771e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6e47248c56446a8a21b512639cc1927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
